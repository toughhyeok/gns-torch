import os

import torch
import torch.nn.functional as F
from absl import app
from absl import flags
from absl import logging
from torch_geometric.loader import DataLoader

from dataset import GNSDataset
from graph_network import GNS

flags.DEFINE_string('data_path', None, help='The dataset directory.')
flags.DEFINE_integer('batch_size', 2, help='The batch size.')
flags.DEFINE_integer('num_epochs', 1, help='Number of epochs of training.')
flags.DEFINE_string('model_path', None,
                    help=('The path for saving checkpoints of the model. '
                          'Defaults to a temporary directory.'))
flags.DEFINE_string('output_path', None,
                    help='The path for saving outputs (e.g. rollouts).')

FLAGS = flags.FLAGS
LR = 1e-4


def main(_):
    device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')
    logging.info(f"Using device: {device}")

    # -----------------------------------------
    # 2. ë°ì´í„° & ëª¨ë¸ ì¤€ë¹„
    # -----------------------------------------
    train_dataset = GNSDataset(data_dir=os.path.join(FLAGS.data_path, 'train'), window_length=7, mode='train')

    # ë¡œë” (PyG ë¡œë” ì‚¬ìš©!)
    train_loader = DataLoader(
        train_dataset,
        batch_size=FLAGS.batch_size,
        shuffle=True,
        num_workers=4
    )

    # ëª¨ë¸ ì´ˆê¸°í™”
    model = GNS(input_dim=2, hidden_size=128, num_layers=10, radius=0.015).to(device)

    # ìµœì í™” ë„êµ¬
    optimizer = torch.optim.Adam(model.parameters(), lr=LR)

    # -----------------------------------------
    # 3. í•™ìŠµ ë£¨í”„ (Training Loop)
    # -----------------------------------------
    logging.info("ğŸš€ í•™ìŠµ ì‹œì‘!")
    model.train()

    epoch = batch_idx = 0

    for epoch in range(FLAGS.num_epochs):
        total_loss = 0
        num_batches = 0

        for batch_idx, batch in enumerate(train_loader):
            batch = batch.to(device)

            # --- [ì¤‘ìš”] íƒ€ê²Ÿ ê°€ì†ë„(Ground Truth Acceleration) ê³„ì‚° ---
            # ë°ì´í„°ì…‹ì€ 'ë‹¤ìŒ ìœ„ì¹˜(y)'ë¥¼ ì¤ë‹ˆë‹¤.
            # í•˜ì§€ë§Œ ìš°ë¦¬ëŠ” 'ê°€ì†ë„'ë¥¼ ë§ì¶°ì•¼ í•˜ë¯€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
            # ê°€ì†ë„ = ë‹¤ìŒìœ„ì¹˜ - í˜„ì¬ìœ„ì¹˜ - í˜„ì¬ì†ë„
            # a_t = p_{t+1} - p_t - v_t
            #     = p_{t+1} - 2*p_t + p_{t-1}

            next_pos = batch.y  # p_{t+1} (Target)
            curr_pos = batch.x[:, -1]  # p_t (Current)
            prev_pos = batch.x[:, -2]  # p_{t-1} (Previous)

            # ì •ë‹µ ê°€ì†ë„ ê³„ì‚°
            # (ì£¼ì˜: ë…¸ì´ì¦ˆê°€ ì„ì¸ ì…ë ¥ ê¸°ì¤€ìœ¼ë¡œ ê°€ì†ë„ë¥¼ ê³„ì‚°í•´ì•¼ ëª¨ë¸ì´ ë…¸ì´ì¦ˆ ë³´ì •ì„ ë°°ì›ë‹ˆë‹¤)
            current_vel = curr_pos - prev_pos
            target_acc = next_pos - curr_pos - current_vel

            # --- Forward & Backward ---
            optimizer.zero_grad()

            # ëª¨ë¸ ì˜ˆì¸¡ (pred_acc)
            pred_acc = model(batch)

            # Loss ê³„ì‚° (ê°€ì†ë„ ë¼ë¦¬ ë¹„êµ)
            loss = F.mse_loss(pred_acc, target_acc)

            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            if batch_idx % 100 == 0:
                logging.info(f"Epoch {epoch + 1} | Batch {batch_idx} | Loss: {loss.item():e}")

            if (epoch * len(train_loader) + batch_idx) % 5000 == 0:
                torch.save(model.state_dict(),
                           os.path.join(FLAGS.model_path, f"gns_model_{epoch + 1}_{batch_idx + 1}.pth"))

        avg_loss = total_loss / num_batches
        logging.info(f"=== Epoch {epoch + 1} Done. Avg Loss: {avg_loss:.6f} ===\n")

    logging.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")

    # -----------------------------------------
    # 4. ëª¨ë¸ ì €ì¥
    # -----------------------------------------
    torch.save(model.state_dict(), os.path.join(FLAGS.model_path, f"gns_model_{epoch + 1}_{batch_idx + 1}.pth"))
    logging.info("ëª¨ë¸ ì €ì¥ ì™„ë£Œ: gns_model_water.pth")


if __name__ == '__main__':
    app.run(main)
